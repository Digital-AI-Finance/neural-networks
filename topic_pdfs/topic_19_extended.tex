\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}

% Apply custom colors
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}
\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

\setbeamertemplate{navigation symbols}{}
\setbeamersize{text margin left=5mm,text margin right=5mm}

\title{19. Confusion Matrix}
\subtitle{Neural Networks - From Brain to Business}
\date{}

\begin{document}

\begin{frame}[plain]
\titlepage
\end{frame}

\begin{frame}{Learning Goal}
Evaluate classifier performance beyond simple accuracy using precision, recall, and F1 score.
\end{frame}

\begin{frame}{Key Concept (1/2)}
A \textbf{confusion matrix} breaks down predictions into four categories:
- \textbf{True Positive (TP)}: Predicted UP, actually UP
- \textbf{False Positive (FP)}: Predicted UP, actually DOWN (Type I error)
- \textbf{True Negative (TN)}: Predicted DOWN, actually DOWN
- \textbf{False Negative (FN)}: Predicted DOWN, actually UP (Type II error)
\end{frame}

\begin{frame}{Key Concept (2/2)}
This granular view reveals important patterns that overall accuracy hides:
- \textbf{Precision}: When we predict UP, how often are we right?
- \textbf{Recall}: Of all actual UP days, how many did we catch?
- \textbf{F1 Score}: Harmonic mean of precision and recall

In trading, these distinctions matter: a false BUY signal (FP) costs money on a losing trade, while a missed opportunity (FN) is a foregone profit. Different strategies prioritize different metrics.
\end{frame}

\begin{frame}{Visualization}
\begin{center}
\includegraphics[width=0.9\textwidth,height=0.75\textheight,keepaspectratio]{19_confusion_matrix/confusion_matrix.pdf}
\end{center}
\end{frame}

\begin{frame}{Key Formula}
\textbf{Precision} (positive predictive value):
\[\text{Precision} = \frac{TP}{TP + FP}\]

\textbf{Recall} (sensitivity, true positive rate):
\[\text{Recall} = \frac{TP}{TP + FN}\]

\textbf{F1 Score} (harmonic mean):
\[F_1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}\]

\textbf{Accuracy}:
\[\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}\]
\end{frame}

\begin{frame}{Intuitive Explanation}
Think of a spam filter:

- \textbf{High precision}: When it says "spam," it's almost always right. Few legitimate emails are blocked.
- \textbf{High recall}: It catches almost all spam. Few spam emails get through.

You can't maximize both simultaneously:
- More aggressive filtering = higher recall, lower precision
- More conservative filtering = higher precision, lower recall

In trading:
- High precision strategy: Trade rarely, but when you trade, usually win
- High recall strategy: Capture most winning opportunities, but also some losers
\end{frame}

\begin{frame}{Practice Problem 1}
\textbf{Problem 1}

Given: TP = 40, FP = 15, TN = 35, FN = 10. Calculate precision, recall, and F1 score.


\vspace{1em}
\begin{block}{Solution}
\small

\textbf{Precision:}
\[\text{Precision} = \frac{TP}{TP + FP} = \frac{40}{40 + 15} = \frac{40}{55} = 0.727 = 72.7\%\]

\textbf{Recall:}
\[\text{Recall} = \frac{TP}{TP + FN} = \frac{40}{40 + 10} = \frac{40}{50} = 0.800 = 80.0\%\]

\textbf{F1 Score:}
\[F_1 = 2 \times \frac{0.727 \times 0.800}{0.727 + 0.800} = 2 \times \frac{0.582}{1.527} = 0.762 = 76.2\%\]

\textbf{Accuracy (for reference):}
\[\text{Accuracy} = \frac{40 + 35}{40 + 15 + 35 + 10} = \frac{75}{100} = 75\%\]



\end{block}
\end{frame}

\begin{frame}{Practice Problem 2}
\textbf{Problem 2}

Model A: Precision = 90\%, Recall = 50\%. Model B: Precision = 70\%, Recall = 70\%. Which is better for a conservative trading strategy?


\vspace{1em}
\begin{block}{Solution}
\small

\textbf{Model A is better for conservative trading.}

\textbf{Why:}

\textbf{Model A (High precision):}
- When it says BUY, it's right 90\% of the time
- Only catches 50\% of opportunities
- Trades less frequently but with higher confidence
- Lower risk of losing trades

\textbf{Model B (Balanced):}
- When it says BUY, it's right 70\% of the time
- Catches 70\% of opportunities
- More trades, more wrong signals
- Higher risk but also catches more winners

\textbf{F1 comparison:}
- Model A: $F_1 = 2 \times \frac{0.90 \times 0.50}{0.90 + 0.50} = 0.643$
- Model B: $F_1 = 2 \times \frac{0.70 \times 0.70}{0.70 + 0.70} = 0.700$

By F1, Model B is "better" overall. But for conservative trading that prioritizes avoiding losses, Model A's 90\% precision is preferable.

\textbf{Business context determines the right choice.}



\end{block}
\end{frame}

\begin{frame}{Key Takeaways}
\begin{itemize}
  \item Confusion matrix reveals four types of prediction outcomes
  \item Accuracy alone can be misleading (especially with imbalanced classes)
  \item Precision: "When I predict positive, am I right?"
  \item Recall: "Do I catch all the actual positives?"
  \item F1 balances precision and recall
  \item Choose metrics based on business costs of different errors
\end{itemize}
\end{frame}

\end{document}
