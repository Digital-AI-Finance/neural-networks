\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}

% Additional colors for template compatibility
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}

\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Clean itemize/enumerate
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]

% Reduce margins for more content space
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Command for bottom annotation (Madrid-style)
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

\begin{document}

\begin{frame}[plain]
\titlepage
\end{frame}

\begin{frame}{Learning Goal}
Compare network performance before and after training to understand the impact of learning.

\bottomnote{This slide establishes the learning objective for this topic}
\end{frame}

\begin{frame}{Key Concept (1/2)}
Before training, a neural network with random weights is essentially guessing. For binary classification (up/down), random guessing yields about \textbf{50\% accuracy} - no better than flipping a coin.

After training on historical data, the network learns patterns that improve predictions. A well-trained network might achieve \textbf{70\% accuracy} on unseen test data - significantly better than chance.

\bottomnote{Understanding this concept is crucial for neural network fundamentals}
\end{frame}

\begin{frame}{Key Concept (2/2)}
The improvement from 50\% to 70\% represents real predictive power extracted from the data. However, it's crucial to evaluate on \textbf{held-out test data} that wasn't used during training. Performance on training data is often inflated (the network may have memorized specifics rather than learning generalizable patterns).

Even modest accuracy improvements can be valuable in finance, where small edges compound over many trades.

\bottomnote{Understanding this concept is crucial for neural network fundamentals}
\end{frame}

\begin{frame}{Visualization}
\begin{center}
\includegraphics[width=0.9\textwidth,height=0.75\textheight,keepaspectratio]{18_prediction_results/prediction_results.pdf}
\end{center}

\bottomnote{Visual representations help solidify abstract concepts}
\end{frame}

\begin{frame}{Key Formula}
\textbf{Accuracy:}
\[\text{Accuracy} = \frac{\text{Correct Predictions}}{\text{Total Predictions}} \times 100\%\]

\textbf{Improvement over baseline:}
\[\text{Improvement} = \text{Trained Accuracy} - \text{Random Baseline}\]

For binary classification:
\[\text{Random Baseline} = 50\%\]

\bottomnote{Mathematical formalization provides precision}
\end{frame}

\begin{frame}{Intuitive Explanation}
Imagine two weather forecasters:
- \textbf{Forecaster A} (random): Flips a coin to predict rain or shine
- \textbf{Forecaster B} (trained): Studies historical weather patterns

Over 100 days:
- Forecaster A: ~50 correct (coin flip)
- Forecaster B: ~70 correct (learned patterns)

Forecaster B adds real value by capturing patterns that random guessing misses. Similarly, a trained neural network extracts predictive information from data that raw chance cannot access.

\bottomnote{Intuitive explanations bridge theory and practice}
\end{frame}

\begin{frame}{Practice Problem 1}
\textbf{Problem 1}

A trained network makes 140 correct predictions out of 200 test samples. Calculate: (a) accuracy, (b) improvement over random baseline.


\vspace{1em}
\begin{block}{Solution}
\small

\textbf{(a) Accuracy:}
\[\text{Accuracy} = \frac{140}{200} \times 100\% = 70\%\]

\textbf{(b) Improvement over baseline:}
\[\text{Improvement} = 70\% - 50\% = 20\%\]

The network predicts \textbf{20 percentage points} better than random guessing - a substantial improvement that could translate to profitable trading.



\end{block}

\bottomnote{Practice problems reinforce understanding}
\end{frame}

\begin{frame}{Practice Problem 2}
\textbf{Problem 2}

Network A achieves 95\% accuracy on training data but 52\% on test data. Network B achieves 72\% on training data and 68\% on test data. Which network is better?


\vspace{1em}
\begin{block}{Solution}
\small

\textbf{Network B is better}, despite lower training accuracy.

Analysis:

\textbf{Network A:}
- Training: 95\%, Test: 52\%
- Gap: 43 percentage points
- Severe \textbf{overfitting} - memorized training data
- Test accuracy barely better than chance (52\% vs 50\%)

\textbf{Network B:}
- Training: 72\%, Test: 68\%
- Gap: 4 percentage points
- Good \textbf{generalization} - learned transferable patterns
- Test accuracy significantly better than chance

Network B's 68\% test accuracy represents real predictive power. Network A's 95\% training accuracy is misleading - it fails on new data.



\end{block}

\bottomnote{Practice problems reinforce understanding}
\end{frame}

\begin{frame}{Key Takeaways}
\begin{itemize}
  \item Random guessing yields ~50\% accuracy on binary classification
  \item Training extracts patterns that improve beyond baseline
  \item Always evaluate on held-out test data, not training data
  \item Even modest improvements (55-70\%) can be valuable in finance
  \item Gap between training and test accuracy indicates overfitting
\end{itemize}

\bottomnote{These key points summarize the essential learnings}
\end{frame}

\end{document}
