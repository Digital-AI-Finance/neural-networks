{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Prediction with Neural Networks\n",
    "\n",
    "## Educational Notebook - Building Understanding from Scratch\n",
    "\n",
    "This notebook demonstrates neural network concepts by building a real stock price prediction model **step by step**, starting with the absolute simplest version.\n",
    "\n",
    "**Learning Philosophy:**\n",
    "1. **Start Simple**: Single neuron → Full network\n",
    "2. **Understand Before Training**: See what untrained networks do\n",
    "3. **Evaluation First**: Measure performance, then improve\n",
    "4. **Build Intuition**: No black boxes, see every calculation\n",
    "\n",
    "**Concepts from Slides:**\n",
    "- Neuron Function (Slide 2)\n",
    "- Activation Functions (Slides 3-4)\n",
    "- Forward Propagation (Slide 6)\n",
    "- Loss Calculation (Slide 7)\n",
    "- Gradient Descent (Slide 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install yfinance pandas numpy matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading AAPL stock data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['AAPL']: JSONDecodeError('Expecting value: line 1 column 1 (char 0)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 0 days of data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m prices \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m days of data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrice range: $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprices\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprices\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Visualize\u001b[39;00m\n\u001b[0;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:45\u001b[0m, in \u001b[0;36m_amin\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_amin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_minimum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation minimum which has no identity"
     ]
    }
   ],
   "source": [
    "# Download Apple stock data\n",
    "ticker = 'AAPL'\n",
    "print(f\"Downloading {ticker} stock data...\")\n",
    "\n",
    "data = yf.download(ticker, start='2019-01-01', end='2024-12-01', progress=False)\n",
    "prices = data['Close'].values\n",
    "\n",
    "print(f\"Downloaded {len(data)} days of data\")\n",
    "print(f\"Price range: ${prices.min():.2f} - ${prices.max():.2f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(data.index, prices, linewidth=1)\n",
    "plt.title(f'{ticker} Stock Price (2019-2024)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences: Use past N days to predict next day\n",
    "def create_sequences(data, lookback=10):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - lookback):\n",
    "        X.append(data[i:i+lookback])\n",
    "        y.append(data[i+lookback])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "lookback = 10\n",
    "X, y = create_sequences(prices, lookback)\n",
    "\n",
    "print(f\"Created {len(X)} training examples\")\n",
    "print(f\"Each input: {lookback} days of prices\")\n",
    "print(f\"Each output: Next day's price\")\n",
    "print(f\"\\nExample: Given {X[0][:3]}... predict ${y[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 70% train, 15% validation, 15% test\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.15 * len(X))\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "print(f\"Train: {len(X_train)} samples\")\n",
    "print(f\"Validation: {len(X_val)} samples\")\n",
    "print(f\"Test: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to [0, 1] range (neural networks work better with normalized data)\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_val_scaled = scaler_X.transform(X_val)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(\"Data normalized to [0, 1] range\")\n",
    "print(f\"Sample input (normalized): {X_train_scaled[0][:3]}...\")\n",
    "print(f\"Sample output (normalized): {y_train_scaled[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Simplest Neural Network - Single Neuron\n",
    "\n",
    "Before building complex networks, let's start with **the absolute simplest version**: one neuron.\n",
    "\n",
    "**Single Neuron Equation (Slide 2):**\n",
    "```\n",
    "y = w * x + b\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `x` = input (one feature)\n",
    "- `w` = weight (how important is the input?)\n",
    "- `b` = bias (baseline prediction)\n",
    "- `y` = output (prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2A: Understanding the Untrained Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity: use only the last day's price to predict next day\n",
    "# (Instead of all 10 days, use just 1 feature)\n",
    "X_train_simple = X_train_scaled[:, -1]  # Last day only\n",
    "X_test_simple = X_test_scaled[:, -1]\n",
    "\n",
    "print(f\"Simplified input: Using only last day's price\")\n",
    "print(f\"Training samples: {len(X_train_simple)}\")\n",
    "print(f\"Test samples: {len(X_test_simple)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ONE neuron with random weight and bias\n",
    "w = np.random.randn() * 0.01  # Small random number\n",
    "b = 0.0\n",
    "\n",
    "print(\"Initialized single neuron (UNTRAINED):\")\n",
    "print(f\"Weight (w) = {w:.6f}\")\n",
    "print(f\"Bias (b) = {b:.6f}\")\n",
    "print(f\"\\nEquation: y = {w:.6f} * x + {b:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Propagation Walkthrough\n",
    "\n",
    "Let's see how the neuron makes a prediction step-by-step with **actual numbers**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take first example from test set\n",
    "x_example = X_test_simple[0]\n",
    "y_true_example = y_test_scaled[0]\n",
    "\n",
    "print(\"Example Prediction (Step-by-Step):\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Input (x):          {x_example:.6f} (normalized price)\")\n",
    "print(f\"Weight (w):         {w:.6f}\")\n",
    "print(f\"Bias (b):           {b:.6f}\")\n",
    "print(f\"\\nCalculation: y = w * x + b\")\n",
    "print(f\"           y = {w:.6f} * {x_example:.6f} + {b:.6f}\")\n",
    "\n",
    "y_pred_example = w * x_example + b\n",
    "print(f\"           y = {y_pred_example:.6f}\")\n",
    "\n",
    "print(f\"\\nActual target (y_true): {y_true_example:.6f}\")\n",
    "print(f\"Prediction (y_pred):    {y_pred_example:.6f}\")\n",
    "print(f\"Error:                  {abs(y_true_example - y_pred_example):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on entire test set (untrained neuron)\n",
    "def predict_single_neuron(X, w, b):\n",
    "    return w * X + b\n",
    "\n",
    "y_pred_untrained = predict_single_neuron(X_test_simple, w, b)\n",
    "\n",
    "print(f\"Made {len(y_pred_untrained)} predictions with untrained neuron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Untrained Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize predictions to actual prices\n",
    "y_pred_untrained_actual = scaler_y.inverse_transform(y_pred_untrained.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(y_test, label='Actual Price', linewidth=2, color='black', alpha=0.7)\n",
    "plt.plot(y_pred_untrained_actual, label='Untrained Neuron Prediction', \n",
    "         linewidth=1.5, linestyle='--', color='red', alpha=0.7)\n",
    "plt.xlabel('Test Sample')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.title('Untrained Single Neuron: Terrible Predictions!', fontweight='bold', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: The predictions are basically random!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Loss (Slide 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error\n",
    "def calculate_mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "mse_untrained = calculate_mse(y_test_scaled, y_pred_untrained)\n",
    "mae_untrained = mean_absolute_error(y_test, y_pred_untrained_actual)\n",
    "rmse_untrained = np.sqrt(mean_squared_error(y_test, y_pred_untrained_actual))\n",
    "\n",
    "print(\"Untrained Neuron Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"MSE (normalized):  {mse_untrained:.6f}\")\n",
    "print(f\"MAE (actual $):    ${mae_untrained:.2f}\")\n",
    "print(f\"RMSE (actual $):   ${rmse_untrained:.2f}\")\n",
    "print(f\"\\nThis is terrible! Let's compare to a baseline...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare to Simple Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive baseline: \"Tomorrow's price = Today's price\"\n",
    "# Use the last day from input as prediction\n",
    "y_baseline = X_test[:, -1]  # Last day's price\n",
    "\n",
    "mae_baseline = mean_absolute_error(y_test, y_baseline)\n",
    "rmse_baseline = np.sqrt(mean_squared_error(y_test, y_baseline))\n",
    "\n",
    "print(\"Baseline (Naive) Performance: 'Tomorrow = Today'\")\n",
    "print(\"=\"*50)\n",
    "print(f\"MAE:  ${mae_baseline:.2f}\")\n",
    "print(f\"RMSE: ${rmse_baseline:.2f}\")\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"Untrained Neuron: MAE=${mae_untrained:.2f}\")\n",
    "print(f\"Naive Baseline:   MAE=${mae_baseline:.2f}\")\n",
    "\n",
    "if mae_untrained > mae_baseline:\n",
    "    print(f\"\\nUntrained neuron is WORSE than naive baseline!\")\n",
    "    print(f\"This makes sense - random weights = random predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2B: Training the Neuron\n",
    "\n",
    "Now let's **train** the neuron using gradient descent (Slide 8).\n",
    "\n",
    "**Goal**: Find better values for `w` and `b` that minimize the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset to random initialization\n",
    "w = np.random.randn() * 0.01\n",
    "b = 0.0\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "# Track loss\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(f\"Training single neuron for {epochs} epochs...\")\n",
    "print(f\"Initial w={w:.6f}, b={b:.6f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Simple training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    y_pred = w * X_train_simple + b\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = calculate_mse(y_train_scaled, y_pred)\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    # Calculate gradients (derivative of MSE)\n",
    "    error = y_pred - y_train_scaled\n",
    "    grad_w = np.mean(error * X_train_simple)\n",
    "    grad_b = np.mean(error)\n",
    "    \n",
    "    # Update weights (gradient descent)\n",
    "    w = w - learning_rate * grad_w\n",
    "    b = b - learning_rate * grad_b\n",
    "    \n",
    "    # Validation loss\n",
    "    y_val_pred = w * X_val_scaled[:, -1] + b\n",
    "    val_loss = calculate_mse(y_val_scaled, y_val_pred)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    if epoch % 100 == 0 or epoch == epochs-1:\n",
    "        print(f\"Epoch {epoch:4d} | Train Loss: {loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"Final w={w:.6f}, b={b:.6f}\")\n",
    "print(f\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_losses, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_losses, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Single Neuron: Learning Progress', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Loss decreased from {train_losses[0]:.6f} to {train_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Trained Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with trained neuron\n",
    "y_pred_trained = predict_single_neuron(X_test_simple, w, b)\n",
    "y_pred_trained_actual = scaler_y.inverse_transform(y_pred_trained.reshape(-1, 1)).flatten()\n",
    "\n",
    "mae_trained = mean_absolute_error(y_test, y_pred_trained_actual)\n",
    "rmse_trained = np.sqrt(mean_squared_error(y_test, y_pred_trained_actual))\n",
    "\n",
    "print(\"Trained Neuron Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"MAE:  ${mae_trained:.2f}\")\n",
    "print(f\"RMSE: ${rmse_trained:.2f}\")\n",
    "\n",
    "print(f\"\\nBefore vs After Training:\")\n",
    "print(f\"Untrained: MAE=${mae_untrained:.2f}\")\n",
    "print(f\"Trained:   MAE=${mae_trained:.2f}\")\n",
    "print(f\"Improvement: ${mae_untrained - mae_trained:.2f}\")\n",
    "\n",
    "print(f\"\\nComparison to Baseline:\")\n",
    "print(f\"Trained Neuron: MAE=${mae_trained:.2f}\")\n",
    "print(f\"Naive Baseline: MAE=${mae_baseline:.2f}\")\n",
    "if mae_trained < mae_baseline:\n",
    "    print(f\"Trained neuron is BETTER than baseline!\")\n",
    "else:\n",
    "    print(f\"Still not better than baseline - need more complexity!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before/After Comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before\n",
    "ax1.plot(y_test, label='Actual', linewidth=2, color='black', alpha=0.7)\n",
    "ax1.plot(y_pred_untrained_actual, label='Untrained', linewidth=1.5, \n",
    "         linestyle='--', color='red', alpha=0.7)\n",
    "ax1.set_xlabel('Test Sample')\n",
    "ax1.set_ylabel('Price ($)')\n",
    "ax1.set_title('BEFORE Training: Random Predictions', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# After\n",
    "ax2.plot(y_test, label='Actual', linewidth=2, color='black', alpha=0.7)\n",
    "ax2.plot(y_pred_trained_actual, label='Trained', linewidth=1.5, \n",
    "         linestyle='--', color='green', alpha=0.7)\n",
    "ax2.set_xlabel('Test Sample')\n",
    "ax2.set_ylabel('Price ($)')\n",
    "ax2.set_title('AFTER Training: Better Predictions', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Full Neural Network (10 → 8 → 1)\n",
    "\n",
    "Now let's build a proper network with:\n",
    "- **Input layer**: 10 neurons (past 10 days)\n",
    "- **Hidden layer**: 8 neurons (with sigmoid activation)\n",
    "- **Output layer**: 1 neuron (prediction)\n",
    "\n",
    "**No classes - just functions and numpy arrays!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3A: Build Untrained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function (Slide 3)\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivative for backpropagation\"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and biases (random, untrained)\n",
    "input_size = 10\n",
    "hidden_size = 8\n",
    "output_size = 1\n",
    "\n",
    "# Layer 1: Input → Hidden\n",
    "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "\n",
    "# Layer 2: Hidden → Output\n",
    "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "print(\"Network initialized (UNTRAINED):\")\n",
    "print(f\"W1 shape: {W1.shape} (connects 10 inputs to 8 hidden neurons)\")\n",
    "print(f\"b1 shape: {b1.shape}\")\n",
    "print(f\"W2 shape: {W2.shape} (connects 8 hidden to 1 output)\")\n",
    "print(f\"b2 shape: {b2.shape}\")\n",
    "print(f\"\\nTotal parameters: {W1.size + b1.size + W2.size + b2.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Propagation (Slide 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Forward propagation through network.\n",
    "    \n",
    "    Returns: output, and intermediate values (for training)\n",
    "    \"\"\"\n",
    "    # Hidden layer\n",
    "    z1 = X @ W1 + b1  # Matrix multiplication\n",
    "    a1 = sigmoid(z1)   # Activation\n",
    "    \n",
    "    # Output layer\n",
    "    z2 = a1 @ W2 + b2  # Matrix multiplication\n",
    "    a2 = z2            # Linear output (no activation for regression)\n",
    "    \n",
    "    return a2, (z1, a1, z2)  # Return output and cache\n",
    "\n",
    "print(\"Forward pass function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction with one example to show step-by-step\n",
    "x_example = X_test_scaled[0:1]  # Take first test sample (shape: 1x10)\n",
    "y_true_example = y_test_scaled[0]\n",
    "\n",
    "print(\"Forward Propagation Step-by-Step:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Input (x): {x_example[0][:3]}... (10 values)\")\n",
    "print(f\"\\nStep 1: Input → Hidden layer\")\n",
    "z1 = x_example @ W1 + b1\n",
    "print(f\"  z1 = X @ W1 + b1\")\n",
    "print(f\"  z1 shape: {z1.shape} (8 values before activation)\")\n",
    "print(f\"  z1 = {z1[0][:3]}...\")\n",
    "\n",
    "a1 = sigmoid(z1)\n",
    "print(f\"\\n  a1 = sigmoid(z1)\")\n",
    "print(f\"  a1 = {a1[0][:3]}... (8 values after sigmoid)\")\n",
    "\n",
    "print(f\"\\nStep 2: Hidden → Output layer\")\n",
    "z2 = a1 @ W2 + b2\n",
    "print(f\"  z2 = a1 @ W2 + b2\")\n",
    "print(f\"  z2 = {z2[0][0]:.6f} (final output)\")\n",
    "\n",
    "print(f\"\\nPrediction: {z2[0][0]:.6f}\")\n",
    "print(f\"Actual:     {y_true_example:.6f}\")\n",
    "print(f\"Error:      {abs(z2[0][0] - y_true_example):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on entire test set (untrained)\n",
    "y_pred_nn_untrained, _ = forward_pass(X_test_scaled, W1, b1, W2, b2)\n",
    "y_pred_nn_untrained = y_pred_nn_untrained.flatten()\n",
    "\n",
    "# Denormalize\n",
    "y_pred_nn_untrained_actual = scaler_y.inverse_transform(y_pred_nn_untrained.reshape(-1, 1)).flatten()\n",
    "\n",
    "mae_nn_untrained = mean_absolute_error(y_test, y_pred_nn_untrained_actual)\n",
    "rmse_nn_untrained = np.sqrt(mean_squared_error(y_test, y_pred_nn_untrained_actual))\n",
    "\n",
    "print(\"Untrained Neural Network Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"MAE:  ${mae_nn_untrained:.2f}\")\n",
    "print(f\"RMSE: ${rmse_nn_untrained:.2f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(y_test, label='Actual', linewidth=2, color='black', alpha=0.7)\n",
    "plt.plot(y_pred_nn_untrained_actual, label='Untrained Network', \n",
    "         linewidth=1.5, linestyle='--', color='red', alpha=0.7)\n",
    "plt.xlabel('Test Sample')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.title('Untrained Neural Network: Still Bad!', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3B: Training the Network\n",
    "\n",
    "Now train using backpropagation (Slide 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(X, y_true, W1, b1, W2, b2, z1, a1, z2, learning_rate):\n",
    "    \"\"\"\n",
    "    Backpropagation - calculate gradients and update weights.\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Output layer gradients\n",
    "    a2 = z2  # Output\n",
    "    dz2 = (a2.flatten() - y_true.flatten()).reshape(-1, 1)\n",
    "    dW2 = (a1.T @ dz2) / m\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "    \n",
    "    # Hidden layer gradients\n",
    "    dz1 = (dz2 @ W2.T) * sigmoid_derivative(z1)\n",
    "    dW1 = (X.T @ dz1) / m\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "    \n",
    "    # Update weights\n",
    "    W1_new = W1 - learning_rate * dW1\n",
    "    b1_new = b1 - learning_rate * db1\n",
    "    W2_new = W2 - learning_rate * dW2\n",
    "    b2_new = b2 - learning_rate * db2\n",
    "    \n",
    "    return W1_new, b1_new, W2_new, b2_new\n",
    "\n",
    "print(\"Backpropagation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset weights\n",
    "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Training loop\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(f\"Training neural network for {epochs} epochs...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    y_pred, (z1, a1, z2) = forward_pass(X_train_scaled, W1, b1, W2, b2)\n",
    "    \n",
    "    # Calculate loss\n",
    "    train_loss = calculate_mse(y_train_scaled, y_pred.flatten())\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Backward pass (update weights)\n",
    "    W1, b1, W2, b2 = backward_pass(X_train_scaled, y_train_scaled, \n",
    "                                    W1, b1, W2, b2, z1, a1, z2, learning_rate)\n",
    "    \n",
    "    # Validation loss\n",
    "    y_val_pred, _ = forward_pass(X_val_scaled, W1, b1, W2, b2)\n",
    "    val_loss = calculate_mse(y_val_scaled, y_val_pred.flatten())\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    if epoch % 100 == 0 or epoch == epochs-1:\n",
    "        print(f\"Epoch {epoch:4d} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_losses, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_losses, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Neural Network: Learning Progress', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate trained network\n",
    "y_pred_nn_trained, _ = forward_pass(X_test_scaled, W1, b1, W2, b2)\n",
    "y_pred_nn_trained = y_pred_nn_trained.flatten()\n",
    "y_pred_nn_trained_actual = scaler_y.inverse_transform(y_pred_nn_trained.reshape(-1, 1)).flatten()\n",
    "\n",
    "mae_nn_trained = mean_absolute_error(y_test, y_pred_nn_trained_actual)\n",
    "rmse_nn_trained = np.sqrt(mean_squared_error(y_test, y_pred_nn_trained_actual))\n",
    "\n",
    "print(\"Trained Neural Network Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"MAE:  ${mae_nn_trained:.2f}\")\n",
    "print(f\"RMSE: ${rmse_nn_trained:.2f}\")\n",
    "\n",
    "print(f\"\\nBefore vs After Training:\")\n",
    "print(f\"Untrained Network: MAE=${mae_nn_untrained:.2f}\")\n",
    "print(f\"Trained Network:   MAE=${mae_nn_trained:.2f}\")\n",
    "print(f\"Improvement: ${mae_nn_untrained - mae_nn_trained:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before/After visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(y_test, label='Actual', linewidth=2, color='black', alpha=0.7)\n",
    "ax1.plot(y_pred_nn_untrained_actual, label='Untrained', linewidth=1.5, \n",
    "         linestyle='--', color='red', alpha=0.7)\n",
    "ax1.set_xlabel('Test Sample')\n",
    "ax1.set_ylabel('Price ($)')\n",
    "ax1.set_title('BEFORE Training', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(y_test, label='Actual', linewidth=2, color='black', alpha=0.7)\n",
    "ax2.plot(y_pred_nn_trained_actual, label='Trained', linewidth=1.5, \n",
    "         linestyle='--', color='green', alpha=0.7)\n",
    "ax2.set_xlabel('Test Sample')\n",
    "ax2.set_ylabel('Price ($)')\n",
    "ax2.set_title('AFTER Training', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "results = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Naive Baseline',\n",
    "        'Single Neuron (Trained)',\n",
    "        'Neural Network 10→8→1 (Trained)'\n",
    "    ],\n",
    "    'MAE ($)': [\n",
    "        mae_baseline,\n",
    "        mae_trained,\n",
    "        mae_nn_trained\n",
    "    ],\n",
    "    'RMSE ($)': [\n",
    "        rmse_baseline,\n",
    "        rmse_trained,\n",
    "        rmse_nn_trained\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_idx = results['MAE ($)'].idxmin()\n",
    "print(f\"\\nBest Model: {results.loc[best_idx, 'Model']}\")\n",
    "print(f\"  MAE: ${results.loc[best_idx, 'MAE ($)']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(y_test, label='Actual Price', linewidth=2.5, alpha=0.8, color='black')\n",
    "plt.plot(y_baseline, label='Baseline (Tomorrow=Today)', \n",
    "         linewidth=1.5, linestyle=':', alpha=0.7)\n",
    "plt.plot(y_pred_trained_actual, label='Single Neuron', \n",
    "         linewidth=1.5, linestyle='--', alpha=0.7)\n",
    "plt.plot(y_pred_nn_trained_actual, label='Neural Network (10→8→1)', \n",
    "         linewidth=2, alpha=0.8)\n",
    "plt.xlabel('Test Sample', fontsize=12)\n",
    "plt.ylabel('AAPL Stock Price ($)', fontsize=12)\n",
    "plt.title('All Models: Comparison on Test Set', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Untrained networks make random predictions** - we saw this clearly with both the single neuron and full network\n",
    "2. **Training improves performance** - gradient descent finds better weights\n",
    "3. **More complexity helps** - the full network (10→8→1) outperformed the single neuron\n",
    "4. **Forward propagation** - we walked through the math step-by-step\n",
    "5. **Backpropagation** - we implemented gradient descent manually\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Start simple**: Single neuron → Full network\n",
    "- **Understand before training**: See what random weights do\n",
    "- **Evaluation first**: Measure, then improve\n",
    "- **No black boxes**: Every calculation is explicit\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- Stock prices are partially random (efficient market hypothesis)\n",
    "- Historical patterns don't guarantee future results\n",
    "- Many external factors not captured in price history alone\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try different features (volume, technical indicators)\n",
    "- Experiment with deeper networks\n",
    "- Add regularization (dropout, L2)\n",
    "- Try LSTM networks (better for time series)\n",
    "- Use professional frameworks (PyTorch - see appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: PyTorch Implementation (Optional)\n",
    "\n",
    "For reference, here's how professionals would implement the same network using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch implementation - kept as reference\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_torch = torch.FloatTensor(X_train_scaled)\n",
    "y_train_torch = torch.FloatTensor(y_train_scaled).unsqueeze(1)\n",
    "X_test_torch = torch.FloatTensor(X_test_scaled)\n",
    "y_test_torch = torch.FloatTensor(y_test_scaled).unsqueeze(1)\n",
    "\n",
    "# Define model\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 8)\n",
    "        self.fc2 = nn.Linear(8, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Train\n",
    "model = SimpleNet()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Training PyTorch model...\")\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train_torch)\n",
    "    loss = criterion(y_pred, y_train_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.6f}\")\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_pt = model(X_test_torch).numpy().flatten()\n",
    "    y_pred_pt_actual = scaler_y.inverse_transform(y_pred_pt.reshape(-1, 1)).flatten()\n",
    "    mae_pt = mean_absolute_error(y_test, y_pred_pt_actual)\n",
    "    print(f\"\\nPyTorch Model MAE: ${mae_pt:.2f}\")\n",
    "    print(f\"Our Implementation MAE: ${mae_nn_trained:.2f}\")\n",
    "    print(f\"\\nBoth achieve similar performance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
