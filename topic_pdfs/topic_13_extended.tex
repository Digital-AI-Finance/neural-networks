\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}

% Apply custom colors
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}
\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

\setbeamertemplate{navigation symbols}{}
\setbeamersize{text margin left=5mm,text margin right=5mm}

\title{13. Loss Landscape}
\subtitle{Neural Networks - From Brain to Business}
\date{}

\begin{document}

\begin{frame}[plain]
\titlepage
\end{frame}

\begin{frame}{Learning Goal}
Visualize the error surface that neural networks navigate during training.
\end{frame}

\begin{frame}{Key Concept (1/3)}
The \textbf{loss function} measures how wrong our predictions are. For each possible combination of weights, we can calculate the total error across all training examples. If we plot this error as a function of weights, we get the \textbf{loss landscape} - a surface with hills (high error) and valleys (low error).
\end{frame}

\begin{frame}{Key Concept (2/3)}
Training a neural network is like finding the lowest point in this landscape. The \textbf{global minimum} is the lowest point overall - the best possible weights. Local minima are lower points surrounded by higher ground, but not the absolute lowest.

With only two weights, we can visualize this as a 3D surface. With millions of weights (typical for modern networks), the landscape exists in millions of dimensions - impossible to visualize but mathematically identical.
\end{frame}

\begin{frame}{Key Concept (3/3)}
The shape of the loss landscape determines how easy or hard training will be. Smooth landscapes with few local minima are easier to optimize than rugged landscapes with many traps.
\end{frame}

\begin{frame}{Visualization}
\begin{center}
\includegraphics[width=0.9\textwidth,height=0.75\textheight,keepaspectratio]{13_loss_landscape/loss_landscape.pdf}
\end{center}
\end{frame}

\begin{frame}{Key Formula}
\textbf{Binary Cross-Entropy Loss} (for classification):

\[L = -\frac{1}{N}\sum_{i=1}^{N} [y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]\]

Where:
- \textbf{N} = number of training examples
- \(\mathbf{y_i}\) = true label (0 or 1)
- \(\mathbf{y-hat_i}\) = predicted probability
- \textbf{L} = loss value (lower is better)
\end{frame}

\begin{frame}{Intuitive Explanation}
Imagine you're blindfolded on a hilly terrain, trying to find the lowest valley. You can only feel the slope directly beneath your feet. The loss landscape is like this terrain, where:

- Your position = current weight values
- Altitude = prediction error (loss)
- Goal = find the lowest altitude (minimum loss)

The challenge: you might get stuck in a small dip (local minimum) without realizing there's a deeper valley nearby. Training strategies like momentum and learning rate schedules help escape such traps.
\end{frame}

\begin{frame}{Practice Problem 1}
\textbf{Problem 1}

Calculate the binary cross-entropy loss for a single prediction where the true label is y = 1 and the predicted probability is y-hat = 0.9.


\vspace{1em}
\begin{block}{Solution}
\small

\[L = -[y \log(\hat{y}) + (1-y)\log(1-\hat{y})]\]
\[L = -[1 \cdot \log(0.9) + (1-1) \cdot \log(1-0.9)]\]
\[L = -[\log(0.9) + 0]\]
\[L = -(-0.105) = 0.105\]

The loss is \textbf{0.105}. This is low because our prediction (90\%) closely matches the true label (1).



\end{block}
\end{frame}

\begin{frame}{Practice Problem 2}
\textbf{Problem 2}

Now calculate the loss when y = 1 but y-hat = 0.1 (a bad prediction).


\vspace{1em}
\begin{block}{Solution}
\small

\[L = -[1 \cdot \log(0.1) + 0 \cdot \log(0.9)]\]
\[L = -[\log(0.1)]\]
\[L = -(-2.303) = 2.303\]

The loss is \textbf{2.303} - much higher than before! This penalizes confident wrong predictions severely.

Comparison:
- Good prediction (y-hat=0.9): Loss = 0.105
- Bad prediction (y-hat=0.1): Loss = 2.303 (22x worse)



\end{block}
\end{frame}

\begin{frame}{Key Takeaways}
\begin{itemize}
  \item Loss functions quantify prediction error
  \item The loss landscape shows error as a function of weights
  \item Training seeks to find the minimum loss (optimal weights)
  \item Cross-entropy severely penalizes confident wrong predictions
  \item Landscape shape affects how easily we can find good solutions
\end{itemize}
\end{frame}

\end{document}
