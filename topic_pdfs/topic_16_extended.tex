\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}

% Additional colors for template compatibility
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}

\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Clean itemize/enumerate
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]

% Reduce margins for more content space
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Command for bottom annotation (Madrid-style)
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

\begin{document}

\begin{frame}[plain]
\titlepage
\end{frame}

\begin{frame}{Learning Goal}
Understand how learning rate affects training speed and stability.

\bottomnote{This slide establishes the learning objective for this topic}
\end{frame}

\begin{frame}{Key Concept (1/2)}
The \textbf{learning rate} controls how big a step we take during gradient descent. It's one of the most important hyperparameters in neural network training.

\textbf{Too small} (e.g., 0.0001): Training is stable but extremely slow. The network may never reach a good solution in reasonable time.

\textbf{Too large} (e.g., 1.0): Training is unstable. The loss may oscillate wildly or even diverge to infinity. The network overshoots the minimum.

\bottomnote{Understanding this concept is crucial for neural network fundamentals}
\end{frame}

\begin{frame}{Key Concept (2/2)}
\textbf{Just right} (typically 0.001-0.01): Steady progress toward minimum. Fast enough to converge in reasonable time, stable enough not to diverge.

Finding the optimal learning rate often requires experimentation. Techniques like \textbf{learning rate schedules} (reducing rate over time) and \textbf{adaptive methods} (Adam, RMSprop) help automate this process.

\bottomnote{Understanding this concept is crucial for neural network fundamentals}
\end{frame}

\begin{frame}{Visualization}
\begin{center}
\includegraphics[width=0.9\textwidth,height=0.75\textheight,keepaspectratio]{16_learning_rate_comparison/learning_rate_comparison.pdf}
\end{center}

\bottomnote{Visual representations help solidify abstract concepts}
\end{frame}

\begin{frame}{Key Formula}
\textbf{Weight update:}
\[w := w - \eta \cdot \nabla L\]

\textbf{Effect of learning rate:}
- Small eta: \(\Delta w\) is small, slow movement
- Large eta: \(\Delta w\) is large, fast but potentially unstable movement

\textbf{Common learning rate schedules:}

Step decay:
\[\eta_t = \eta_0 \times \gamma^{\lfloor t/k \rfloor}\]

Exponential decay:
\[\eta_t = \eta_0 \times e^{-\lambda t}\]

\bottomnote{Mathematical formalization provides precision}
\end{frame}

\begin{frame}{Intuitive Explanation}
Imagine walking downhill with a blindfold:

- \textbf{Too small step}: You inch forward, taking forever to reach the bottom
- \textbf{Too large step}: You leap so far you might land on the opposite hill
- \textbf{Just right}: Steady steps that make good progress without losing balance

The terrain (loss landscape) determines what "just right" means. Steeper hills allow smaller steps; flat areas can use larger steps.

\bottomnote{Intuitive explanations bridge theory and practice}
\end{frame}

\begin{frame}{Practice Problem 1}
\textbf{Problem 1}

You train with learning rate 0.1 and observe: Loss at epoch 1 = 0.8, epoch 2 = 0.5, epoch 3 = 0.4, epoch 4 = 0.38, epoch 5 = 0.37. What pattern do you see?


\vspace{1em}
\begin{block}{Solution}
\small

\textbf{Pattern: Diminishing returns (normal convergence)}

Analysis:
- Epoch 1->2: Improvement of 0.3 (37.5\% reduction)
- Epoch 2->3: Improvement of 0.1 (20\% reduction)
- Epoch 3->4: Improvement of 0.02 (5\% reduction)
- Epoch 4->5: Improvement of 0.01 (2.6\% reduction)

This is \textbf{healthy training behavior}:
- Rapid early progress (large gradients far from minimum)
- Slowing improvement (smaller gradients near minimum)
- Convergence to stable value

The learning rate (0.1) appears appropriate for this problem.



\end{block}

\bottomnote{Practice problems reinforce understanding}
\end{frame}

\begin{frame}{Practice Problem 2}
\textbf{Problem 2}

You train with learning rate 1.0 and observe: Loss at epoch 1 = 0.8, epoch 2 = 1.5, epoch 3 = 3.2, epoch 4 = 8.7, epoch 5 = NaN. What happened?


\vspace{1em}
\begin{block}{Solution}
\small

\textbf{Diagnosis: Learning rate too high - divergence}

What happened:
- Epoch 1->2: Loss increased (0.8 -> 1.5) - overshot minimum
- Epoch 2->3: Continued increasing - oscillating past optimal
- Epoch 3->4: Accelerating growth - unstable regime
- Epoch 5: NaN (Not a Number) - loss exploded to infinity

\textbf{Why:}
- Large eta means large weight updates
- Updates so large they jump past the minimum to higher loss regions
- Positive feedback: higher loss -> larger gradient -> even larger update
- Eventually overflows numerical precision (NaN)

\textbf{Fix:} Reduce learning rate significantly (try 0.01 or 0.001)



\end{block}

\bottomnote{Practice problems reinforce understanding}
\end{frame}

\begin{frame}{Key Takeaways}
\begin{itemize}
  \item Learning rate is critical: too small = slow, too large = unstable
  \item Look for diminishing returns pattern = healthy training
  \item Look for increasing loss = learning rate too high
  \item Learning rate schedules reduce rate over time
  \item Adaptive optimizers (Adam) adjust rate automatically
\end{itemize}

\bottomnote{These key points summarize the essential learnings}
\end{frame}

\end{document}
